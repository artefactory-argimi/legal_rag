# Legal RAG Evaluation Pipeline - ColBERT Legal French
#
# This pipeline evaluates retrieval quality using pyserini.
#
# Prerequisites:
#   Run colbert_legal_french_index.yaml first to build the index and generate run/qrels files.
#
# Usage:
#   task-tracker submit pipelines/colbert_legal_french_evaluation.yaml

name: colbert_legal_french_evaluation
description: Evaluate ColBERT Legal French retrieval with pyserini
version: "2.0"

project: legal-rag
experiment: legal_rag

defaults:
  base_dir: /data/workspace/${USER}

  # Paths
  output_dir: ${base_dir}/legal-rag/evaluation/colbert_legal_french

steps:
  # Evaluate with pyserini (run.txt and qrels.txt are generated by the index pipeline)
  - command: "python -m pyserini.eval.trec_eval -c -m recall.1,5,10,100 ${output_dir}/qrels.txt ${output_dir}/run.txt"
    artifacts:
      inputs:
        - path: "${output_dir}/run.txt"
          type: Dataset
        - path: "${output_dir}/qrels.txt"
          type: Dataset
      outputs:
        - path: "${output_dir}/metrics.txt"
          type: Metrics
