# Question Generation Pipeline
#
# This pipeline generates question/answer pairs from legal documents
# using an LLM to extract facts and create retriever-friendly questions.
#
# Usage:
#   task-tracker submit pipelines/generate_questions.yaml

name: generate_questions
description: Generate Q&A pairs from legal documents for retrieval evaluation
version: "1.1"

project: legal-rag
experiment: legal_rag

defaults:
  base_dir: /data/workspace/${USER}

  # Dataset configuration
  dataset: artefactory/Argimi-Legal-French-Jurisprudence
  subset: constit
  split: train
  text_column: content
  doc_id_column: id

  # LLM server configuration
  llm_model: openai/gpt-oss-120b
  server_host: 0.0.0.0
  server_port: 8000
  dtype: bfloat16
  tp_size: 4
  mem_fraction_static: 0.92
  chunked_prefill_size: 4096
  max_prefill_tokens: 16384
  max_running_requests: 256

  # LLM client configuration
  api_base: http://localhost:${server_port}/v1
  api_key: local
  model: local
  temperature: 0.2
  max_tokens: 32768

  # Generation configuration
  seed: 42
  max_context_chars: 8000
  num_threads: 16
  log_every: 50

  # Paths
  output_dir: ${base_dir}/legal-rag/questions/${subset}

steps:
  # Step 1: Launch the sglang LLM server
  - command: "uvx sglang serve"
    arguments:
      model-path: "${llm_model}"
      host: "${server_host}"
      port: ${server_port}
      dtype: "${dtype}"
      tp-size: ${tp_size}
      mem-fraction-static: ${mem_fraction_static}
      chunked-prefill-size: ${chunked_prefill_size}
      max-prefill-tokens: ${max_prefill_tokens}
      max-running-requests: ${max_running_requests}
      trust-remote-code: true
    gpu_count: 4
    artifacts:
      inputs: []
      outputs: []

  # Step 2: Generate questions using the LLM
  - command: "python scripts/generate_question.py"
    arguments:
      dataset: "${dataset}"
      config: "${subset}"
      split: "${split}"
      text_column: "${text_column}"
      doc_id_column: "${doc_id_column}"
      api_base: "${api_base}"
      api_key: "${api_key}"
      model: "${model}"
      temperature: ${temperature}
      max_tokens: ${max_tokens}
      seed: ${seed}
      max_context_chars: ${max_context_chars}
      num_threads: ${num_threads}
      log_every: ${log_every}
      output_dir: "${output_dir}"
    gpu_count: 0
    artifacts:
      inputs:
        - path: "${dataset}"
          type: Dataset
      outputs:
        - path: "${output_dir}"
          type: Dataset
