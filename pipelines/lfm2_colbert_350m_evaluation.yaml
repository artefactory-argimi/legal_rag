# Legal RAG Evaluation Pipeline - LFM2-ColBERT-350M
#
# This pipeline evaluates retrieval quality using pyserini.
#
# Prerequisites:
#   Run lfm2_colbert_350m_index.yaml first to build the index and generate run/qrels files.
#
# Usage:
#   task-tracker submit pipelines/lfm2_colbert_350m_evaluation.yaml

name: lfm2_colbert_350m_evaluation
description: Evaluate LFM2-ColBERT-350M retrieval with pyserini
version: "2.0"

project: legal-rag
experiment: legal_rag

defaults:
  base_dir: /data/workspace/${USER}

  # Paths
  output_dir: ${base_dir}/legal-rag/evaluation/lfm2_colbert_350m

steps:
  # Evaluate with pyserini (run.txt and qrels.txt are generated by the index pipeline)
  - command: "python -m pyserini.eval.trec_eval -c -m recall.1,5,10,100 ${output_dir}/qrels.txt ${output_dir}/run.txt"
    artifacts:
      inputs:
        - path: "${output_dir}/run.txt"
          type: Dataset
        - path: "${output_dir}/qrels.txt"
          type: Dataset
      outputs:
        - path: "${output_dir}/metrics.txt"
          type: Metrics
