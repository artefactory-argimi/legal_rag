{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b7010a4",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# French Legal Agent Demo\n",
    "\n",
    "Colab-ready notebook (py:percent via Jupytext) for running the French Legal RAG\n",
    "agent. It wires the DSPy agent, and lets you ask one or many questions. The LM can\n",
    "run via Hugging Face Serverless Inference (with `HF_TOKEN`) or a local\n",
    "OpenAI-compatible server (provide `GENERATOR_API_BASE`). DSPy (via LiteLLM)\n",
    "auto-switches providers based on whether an API base is provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f32d89",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Configuration (Colab form)\n",
    "\n",
    "All editable constants live here. Colab renders `@param` comments as form fields.\n",
    "Tokens can come from login (`interpreter_login`) or manual entry; other fields use\n",
    "these form values. Set `GENERATOR_API_KEY` to your own HF token, or point\n",
    "`GENERATOR_API_BASE` to your OpenAI-compatible server to bypass HF serverless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a016ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATOR_API_KEY = \"local\"  # @param {type:\"string\"}\n",
    "GENERATOR_API_BASE = \"http://localhost:8000/v1\"  # @param {type:\"string\"}\n",
    "GENERATOR_MODEL_ID = (\n",
    "    \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"  # @param {type:\"string\"}\n",
    ")\n",
    "# Encoder zip (URL/path) and extracted path must match the model used for indexing.\n",
    "ENCODER_ZIP_URI = \"https://github.com/artefactory-argimi/legal_rag/releases/download/data-juri-v1/colbert-encoder.zip\"  # @param {type:\"string\"}\n",
    "ENCODER_MODEL_PATH = \"./encoder_model\"  # path where the encoder zip will be extracted (model files at root)\n",
    "# Index zip (URL/path) built offline from the same encoder.\n",
    "INDEX_ZIP_URI = \"https://github.com/artefactory-argimi/legal_rag/releases/download/data-juri-v1/index.zip\"  # @param {type:\"string\"}\n",
    "SEARCH_K = 5  # @param {type:\"integer\"}\n",
    "MAX_NEW_TOKENS = 512  # @param {type:\"integer\"}\n",
    "TEMPERATURE = 0.2  # @param {type:\"number\"}\n",
    "MAX_ITERS = 4  # @param {type:\"integer\"}\n",
    "INSTRUCTIONS = (\n",
    "    \"Tu es un agent RAG spécialisé en jurisprudence française (jeu de données artefactory/Argimi-Legal-French-Jurisprudence). \"\n",
    "    \"Pour chaque question, appelle d'abord search_legal_docs pour trouver des décisions puis lookup_legal_doc pour lire les textes en intégralité. \"\n",
    "    \"Chaque réponse doit citer explicitement la jurisprudence utilisée (titre ou référence) et la date de la décision. \"\n",
    "    \"Présente d'abord les éléments juridiques pertinents (faits, fondement, dispositif, articles cités), puis formule une réponse synthétique. \"\n",
    "    \"La réponse doit être une interprétation fondée uniquement sur les décisions récupérées, jamais sur ta mémoire du modèle. \"\n",
    "    \"Si aucune décision pertinente n'est récupérée ou si les éléments ne permettent pas de répondre, indique clairement que tu n'as pas les informations nécessaires pour répondre à la question. \"\n",
    "    \"Réponds en français de façon précise et utile.\"\n",
    ")  # @param {type:\"string\"}\n",
    "configured_index = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059d594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Ensure the package is installed before importing.\n",
    "REPO_URL = \"https://github.com/artefactory-argimi/legal_rag.git\"  # change if you fork\n",
    "try:\n",
    "    import legal_rag as _  # noqa: F401\n",
    "except ImportError:\n",
    "    try:\n",
    "        get_ipython().run_line_magic(  # type: ignore[name-defined]\n",
    "            \"pip\",\n",
    "            f\"install --quiet --upgrade git+{REPO_URL}\",\n",
    "        )\n",
    "    except Exception as exc:  # pragma: no cover\n",
    "        raise RuntimeError(\"Failed to install legal_rag via %pip; please install manually.\") from exc\n",
    "    import legal_rag as _  # noqa: F401\n",
    "\n",
    "from etils import epath\n",
    "from legal_rag.assets import prepare_assets\n",
    "from legal_rag.assets import prepare_assets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afb73b6",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Encoder and index assets (load first)\n",
    "Provide zipped assets (local path or URL, e.g., GitHub release assets). The\n",
    "encoder zip must contain the ColBERT checkpoint used for indexing. The index zip\n",
    "must contain the PLAID index (e.g., an `index/` folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dceab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run asset prep early so downstream cells only depend on local paths.\n",
    "encoder_path, configured_index = prepare_assets(\n",
    "    encoder_zip_uri=ENCODER_ZIP_URI,\n",
    "    index_zip_uri=INDEX_ZIP_URI,\n",
    "    encoder_dest=Path(\"./encoder_model\"),\n",
    "    index_dest=Path(\"./index\"),\n",
    ")\n",
    "print(f\"✓ Encoder ready at {encoder_path}\")\n",
    "print(f\"✓ Index ready at {configured_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c478a1",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Hugging Face login (Serverless Inference)\n",
    "\n",
    "If running without a local generator and using the Hugging Face provider without\n",
    "an `HF_TOKEN` set, prompt for a token using `huggingface_hub.interpreter_login()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd291063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import get_token, interpreter_login\n",
    "\n",
    "if not GENERATOR_API_BASE and not GENERATOR_API_KEY:\n",
    "    # Default to HF serverless; prompt for token once if none was supplied.\n",
    "    interpreter_login()\n",
    "    GENERATOR_API_KEY = get_token() or \"\"\n",
    "\n",
    "# Increase HF download timeout to reduce transient failures when fetching models.\n",
    "os.environ.setdefault(\"HF_HUB_TIMEOUT\", \"60\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5651d3de",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Encoder ↔ Index coupling\n",
    "The encoder loaded here must be the same model (or local snapshot path) used when\n",
    "building the index. If you indexed with a different ColBERT checkpoint, update\n",
    "the encoder assets accordingly to avoid mismatched embeddings.\n",
    "\n",
    "## Agent configuration\n",
    "We build the DSPy ReAct agent using the helpers in `agent.py`.\n",
    "\n",
    "- Encoder: local, GPU if available (`torch.cuda.is_available()`), no API keys.\n",
    "- Generator: defaults to Hugging Face Serverless (`huggingface/<model>` with token from\n",
    "  `interpreter_login`) and falls back to a local OpenAI-compatible server when\n",
    "  `GENERATOR_API_BASE` is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06d5628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from legal_rag.agent import build_agent\n",
    "\n",
    "generator_api_key = GENERATOR_API_KEY or None\n",
    "# If no API base is set, default to HF Serverless and try to pick up a saved token.\n",
    "if not GENERATOR_API_BASE and not generator_api_key:\n",
    "    generator_api_key = get_token()\n",
    "generator_api_base = GENERATOR_API_BASE or None\n",
    "\n",
    "agent = build_agent(\n",
    "    student_model=GENERATOR_MODEL_ID,\n",
    "    encoder_model=encoder_path,\n",
    "    generator_api_key=generator_api_key,\n",
    "    generator_api_base=generator_api_base,\n",
    "    index_folder=configured_index,  # used by ColBERT retriever in agent.py\n",
    "    search_k=SEARCH_K,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    temperature=TEMPERATURE,\n",
    "    instructions=INSTRUCTIONS,\n",
    "    max_iters=MAX_ITERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e359200e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Ask questions\n",
    "Provide a single question as a string or multiple questions as an iterable.\n",
    "The agent will search the index, optionally call lookup, and return grounded\n",
    "answers. Adjust `queries` below and re-run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a485e156",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries: Iterable[str] | str = [\n",
    "    \"Quelles sont les obligations principales de l'employeur en matière de sécurité au travail ?\",\n",
    "    \"Dans quel cas un contrat peut-il être résilié pour imprévision selon le droit français ?\",\n",
    "]\n",
    "\n",
    "if isinstance(queries, str):\n",
    "    questions = [queries]\n",
    "else:\n",
    "    questions = list(queries)\n",
    "\n",
    "for idx, question in enumerate(questions, start=1):\n",
    "    print(f\"\\n=== Question {idx} ===\")\n",
    "    print(question)\n",
    "    prediction = agent(question=question)\n",
    "    print(\"\\n--- Réponse ---\")\n",
    "    print(prediction.answer)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\"",
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
