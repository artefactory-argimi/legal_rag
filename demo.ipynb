{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b7010a4",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# French Legal Agent Demo\n",
    "\n",
    "Colab-ready notebook (py:percent via Jupytext) for running the French Legal RAG\n",
    "agent. It wires the DSPy agent, and lets you ask one or many questions. The LM can\n",
    "run via Hugging Face Serverless Inference (with `HF_TOKEN`) or a local\n",
    "OpenAI-compatible server (provide `GENERATOR_API_BASE`). DSPy (via LiteLLM)\n",
    "auto-switches providers based on whether an API base is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059d594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "import zipfile\n",
    "\n",
    "from etils import epath\n",
    "\n",
    "\"\"\"Detect Colab early; avoid hard imports elsewhere.\"\"\"\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f32d89",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Configuration (Colab form)\n",
    "\n",
    "All editable constants live here. Colab renders `@param` comments as form fields.\n",
    "Tokens can come from login (`interpreter_login`) or manual entry; other fields use\n",
    "these form values. Set `GENERATOR_API_KEY` to your own HF token, or point\n",
    "`GENERATOR_API_BASE` to your OpenAI-compatible server to bypass HF serverless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a016ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATOR_API_KEY = \"\"  # @param {type:\"string\"}\n",
    "GENERATOR_API_BASE = \"\"  # @param {type:\"string\"}\n",
    "GENERATOR_MODEL_ID = \"mistralai/Magistral-Small-2509\"  # @param {type:\"string\"}\n",
    "ENCODER_MODEL_ID = \"maastrichtlawtech/colbert-legal-french\"  # @param {type:\"string\"}\n",
    "SEARCH_K = 5  # @param {type:\"integer\"}\n",
    "MAX_NEW_TOKENS = 512  # @param {type:\"integer\"}\n",
    "TEMPERATURE = 0.2  # @param {type:\"number\"}\n",
    "MAX_ITERS = 4  # @param {type:\"integer\"}\n",
    "INSTRUCTIONS = \"First call search_legal_docs to find candidate ids and previews. Then call lookup_legal_doc on specific ids you want to read in full. Ground your answer in the retrieved text and cite the document ids you used.\"  # @param {type:\"string\"}\n",
    "INDEX_PATH = (\n",
    "    \"/content/index\" if IN_COLAB else \"./index\"\n",
    ")  # @param {type:\"string\"}\n",
    "configured_index = epath.Path(INDEX_PATH)\n",
    "index_zip_path = \"\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c478a1",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Hugging Face login (Serverless Inference)\n",
    "\n",
    "If running in Colab and using the Hugging Face provider without an `HF_TOKEN`\n",
    "set, prompt for a token using `huggingface_hub.interpreter_login()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd291063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import get_token, interpreter_login\n",
    "\n",
    "if not GENERATOR_API_BASE and not GENERATOR_API_KEY:\n",
    "    # Default to HF serverless; prompt for token once if none was supplied.\n",
    "    interpreter_login()\n",
    "    GENERATOR_API_KEY = get_token() or \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65333e5",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Upload helper (Colab)\n",
    "\n",
    "If you need to upload a local file (e.g., a zipped index) from your machine into\n",
    "the Colab environment, uncomment and run the cell below. Uploaded filenames and\n",
    "sizes will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ee6738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if IN_COLAB:\n",
    "#     from google.colab import files  # type: ignore\n",
    "#     uploaded = files.upload()\n",
    "#     for fn, data in uploaded.items():\n",
    "#         print(f'User uploaded file \"{fn}\" with length {len(data)} bytes')\n",
    "#     # Auto-handle zip index uploads: save to /content, set index_zip_path, and unzip.\n",
    "#     for fn in uploaded:\n",
    "#         local_path = f\"/content/{fn}\"\n",
    "#         index_zip_path = local_path\n",
    "#         if fn.lower().endswith(\".zip\"):\n",
    "#             import zipfile\n",
    "#             with zipfile.ZipFile(local_path, \"r\") as zf:\n",
    "#                 zf.extractall(\"/content\")\n",
    "#             print(f\"✓ Extracted {fn} to /content\")\n",
    "#         else:\n",
    "#             print(\"Uploaded file is not a zip; please upload a zipped index.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32217f59",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Index loading\n",
    "We use a single configured path for the index. No duplicated paths between local and Colab.\n",
    "\n",
    "If the index folder is missing, you can upload a zipped archive (e.g.\n",
    "`legal_rag_index.zip`) and set `index_zip_path` to its path; the cell below will\n",
    "unzip it into `INDEX_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f119f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not configured_index.exists():\n",
    "    # If an archive path is provided, try to use it.\n",
    "    if index_zip_path:\n",
    "        archive = epath.Path(index_zip_path)\n",
    "        if not archive.exists():\n",
    "            raise FileNotFoundError(f\"Index archive not found: {archive}\")\n",
    "    elif IN_COLAB:\n",
    "        # Prompt for upload when running in Colab and no archive path is given.\n",
    "        from google.colab import files  # type: ignore\n",
    "        uploaded = files.upload()\n",
    "        if not uploaded:\n",
    "            raise FileNotFoundError(\"No index found and no archive uploaded.\")\n",
    "        # Pick the first uploaded file.\n",
    "        fn, data = next(iter(uploaded.items()))\n",
    "        local_path = f\"/content/{fn}\"\n",
    "        index_zip_path = local_path\n",
    "        print(f'User uploaded file \"{fn}\" with length {len(data)} bytes')\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            \"No index found. Provide an index at the configured path or upload a zipped archive.\"\n",
    "        )\n",
    "\n",
    "    archive = epath.Path(index_zip_path)\n",
    "    if not archive.name.lower().endswith(\".zip\"):\n",
    "        raise ValueError(f\"Uploaded/provided file is not a zip archive: {archive}\")\n",
    "\n",
    "    with zipfile.ZipFile(archive, \"r\") as zf:\n",
    "        zf.extractall(\"/content\" if IN_COLAB else str(configured_index.parent))\n",
    "\n",
    "    if not configured_index.exists() and IN_COLAB:\n",
    "        # If the archive contained a folder with a different name, try to locate it.\n",
    "        for child in epath.Path(\"/content\").iterdir():\n",
    "            if child.is_dir() and (child / \"doc_mapping.json\").exists():\n",
    "                configured_index = child\n",
    "                break\n",
    "    if not configured_index.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Archive extracted but index folder missing at {configured_index}\"\n",
    "        )\n",
    "    print(f\"✓ Index extracted to {configured_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25f5622",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Agent configuration\n",
    "We build the DSPy ReAct agent using the helpers in `agent.py`.\n",
    "\n",
    "- Encoder: local, GPU if available (`torch.cuda.is_available()`), no API keys.\n",
    "- Generator: defaults to Hugging Face Serverless (`huggingface/<model>` with token from\n",
    "  `interpreter_login`) and falls back to a local OpenAI-compatible server when\n",
    "  `GENERATOR_API_BASE` is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7d708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import build_agent\n",
    "\n",
    "generator_api_key = GENERATOR_API_KEY or None\n",
    "# If no API base is set, default to HF Serverless and try to pick up a saved token.\n",
    "if not GENERATOR_API_BASE and not generator_api_key:\n",
    "    generator_api_key = get_token()\n",
    "generator_api_base = GENERATOR_API_BASE or None\n",
    "\n",
    "agent = build_agent(\n",
    "    student_model=GENERATOR_MODEL_ID,\n",
    "    encoder_model=ENCODER_MODEL_ID,\n",
    "    generator_api_key=generator_api_key,\n",
    "    generator_api_base=generator_api_base,\n",
    "    mode=None,  # DSPy decides: Hugging Face when no api_base, local server when api_base is set.\n",
    "    index_folder=configured_index,  # used by ColBERT retriever in agent.py\n",
    "    search_k=SEARCH_K,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    temperature=TEMPERATURE,\n",
    "    instructions=INSTRUCTIONS,\n",
    "    max_iters=MAX_ITERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a52e4f8",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Ask questions\n",
    "Provide a single question as a string or multiple questions as an iterable.\n",
    "The agent will search the index, optionally call lookup, and return grounded\n",
    "answers. Adjust `queries` below and re-run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e79884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries: Iterable[str] | str = [\n",
    "    \"Quelles sont les obligations principales de l'employeur en matière de sécurité au travail ?\",\n",
    "    \"Dans quel cas un contrat peut-il être résilié pour imprévision selon le droit français ?\",\n",
    "]\n",
    "\n",
    "if isinstance(queries, str):\n",
    "    questions = [queries]\n",
    "else:\n",
    "    questions = list(queries)\n",
    "\n",
    "for idx, question in enumerate(questions, start=1):\n",
    "    print(f\"\\n=== Question {idx} ===\")\n",
    "    print(question)\n",
    "    prediction = agent(question=question)\n",
    "    print(\"\\n--- Réponse ---\")\n",
    "    print(prediction.answer)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\"",
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
